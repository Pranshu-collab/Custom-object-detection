{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.12.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":2118595,"sourceType":"datasetVersion","datasetId":1271215}],"dockerImageVersionId":31234,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\nimport os\nimport xml.etree.ElementTree as ET\nimport tensorflow as tf\nimport cv2\nfrom sklearn.model_selection import train_test_split\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2026-01-09T16:30:53.469720Z","iopub.execute_input":"2026-01-09T16:30:53.470029Z","iopub.status.idle":"2026-01-09T16:32:40.279284Z","shell.execute_reply.started":"2026-01-09T16:30:53.470001Z","shell.execute_reply":"2026-01-09T16:32:40.278163Z"},"collapsed":true,"jupyter":{"outputs_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"CLASSES = [\"person\", \"car\", \"dog\", \"bicycle\", \"cat\"]\nNUM_CLASSES = len(CLASSES)\nCLASS_TO_ID = {name: i for i, name in enumerate(CLASSES)}\ndef parse_voc_xml(xml_path):\n    tree = ET.parse(xml_path)\n    root = tree.getroot()\n\n    boxes = []\n    labels = []\n\n    for obj in root.findall(\"object\"):\n        cls = obj.find(\"name\").text\n        if cls not in CLASS_TO_ID:\n            continue\n\n        bbox = obj.find(\"bndbox\")\n        xmin = int(float(bbox.find(\"xmin\").text))\n        ymin = int(float(bbox.find(\"ymin\").text))\n        xmax = int(float(bbox.find(\"xmax\").text))\n        ymax = int(float(bbox.find(\"ymax\").text))\n\n        boxes.append([xmin, ymin, xmax, ymax])\n        labels.append(CLASS_TO_ID[cls])\n\n    return np.array(boxes), np.array(labels)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-09T16:40:23.336447Z","iopub.execute_input":"2026-01-09T16:40:23.337610Z","iopub.status.idle":"2026-01-09T16:40:23.344640Z","shell.execute_reply.started":"2026-01-09T16:40:23.337576Z","shell.execute_reply":"2026-01-09T16:40:23.343872Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def load_image_and_labels(img_path, xml_path):\n    image = cv2.imread(img_path)\n    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n\n    boxes, labels = parse_voc_xml(xml_path)\n    return image, boxes, labels\ndef normalize_boxes(boxes, img_w, img_h):\n    boxes = boxes.astype(np.float32)\n    if boxes.size == 0:\n        return np.zeros((0, 4), dtype=np.float32)\n    if boxes.ndim == 1:\n        boxes = np.expand_dims(boxes, axis=0)\n    boxes[:, [0, 2]] /= img_w\n    boxes[:, [1, 3]] /= img_h\n    return boxes","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-09T16:40:27.539605Z","iopub.execute_input":"2026-01-09T16:40:27.539981Z","iopub.status.idle":"2026-01-09T16:40:27.546255Z","shell.execute_reply.started":"2026-01-09T16:40:27.539941Z","shell.execute_reply":"2026-01-09T16:40:27.545444Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"GRID_SIZE = 28\ndef encode_targets(boxes, labels):\n    target = np.zeros((GRID_SIZE, GRID_SIZE, 5 + NUM_CLASSES))\n\n    for box, label in zip(boxes, labels):\n        x_center = (box[0] + box[2]) / 2\n        y_center = (box[1] + box[3]) / 2\n        w = box[2] - box[0]\n        h = box[3] - box[1]\n\n        grid_x = int(x_center * GRID_SIZE)\n        grid_y = int(y_center * GRID_SIZE)\n\n        if grid_x >= GRID_SIZE or grid_y >= GRID_SIZE:\n            continue\n\n        target[grid_y, grid_x, 0:4] = [x_center, y_center, w, h]\n        target[grid_y, grid_x, 4] = 1.0\n        target[grid_y, grid_x, 5 + label] = 1.0\n\n    return target","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-09T16:40:30.198068Z","iopub.execute_input":"2026-01-09T16:40:30.198672Z","iopub.status.idle":"2026-01-09T16:40:30.203465Z","shell.execute_reply.started":"2026-01-09T16:40:30.198642Z","shell.execute_reply":"2026-01-09T16:40:30.202679Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def preprocess(img_path, xml_path):\n    image, boxes, labels = load_image_and_labels(img_path, xml_path)\n    h, w, _ = image.shape\n    if boxes.size > 0:\n        if np.random.rand() < 0.5:\n            image = cv2.flip(image, 1)\n            boxes[:, [0, 2]] = w - boxes[:, [2, 0]]\n\n        if np.random.rand() < 0.5:\n            image = image.astype(np.float32)\n            brightness = np.random.uniform(-30, 30)\n            image += brightness\n            contrast = np.random.uniform(0.8, 1.2)\n            image *= contrast\n            image = np.clip(image, 0, 255)\n            \n    boxes = normalize_boxes(boxes, w, h)\n    if boxes.size == 0:\n      target = np.zeros((28, 28, 5 + NUM_CLASSES), dtype=np.float32)\n    else: \n      target = encode_targets(boxes, labels)\n    image = cv2.resize(image, (224, 224))\n    image = image / 255.0\n    return image.astype(np.float32), target.astype(np.float32)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-09T16:40:33.444601Z","iopub.execute_input":"2026-01-09T16:40:33.445196Z","iopub.status.idle":"2026-01-09T16:40:33.451356Z","shell.execute_reply.started":"2026-01-09T16:40:33.445144Z","shell.execute_reply":"2026-01-09T16:40:33.450551Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"IMAGE_DIR = \"/kaggle/input/pascal-voc-2012-dataset/VOC2012_train_val/VOC2012_train_val/JPEGImages\"\nANNOT_DIR = \"/kaggle/input/pascal-voc-2012-dataset/VOC2012_train_val/VOC2012_train_val/Annotations\"\n\nimage_files = sorted(os.listdir(IMAGE_DIR))\nxml_files = sorted(os.listdir(ANNOT_DIR))\n\ndata_pairs = list(zip(\n    [os.path.join(IMAGE_DIR, f) for f in image_files],\n    [os.path.join(ANNOT_DIR, f.replace(\".jpg\", \".xml\")) for f in image_files]\n))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-09T16:40:36.720752Z","iopub.execute_input":"2026-01-09T16:40:36.721589Z","iopub.status.idle":"2026-01-09T16:40:36.785674Z","shell.execute_reply.started":"2026-01-09T16:40:36.721540Z","shell.execute_reply":"2026-01-09T16:40:36.785116Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train_pairs, val_pairs = train_test_split(\n    data_pairs,\n    test_size=0.2,\n    random_state=42,\n    shuffle=True\n)\noutput_signature=(\n    tf.TensorSpec((224, 224, 3), tf.float32),\n    tf.TensorSpec((28, 28, 5 + NUM_CLASSES), tf.float32)\n)\ndef tf_data_generator(data_pairs):\n    for img_path, xml_path in data_pairs:\n        image, target = preprocess(img_path, xml_path)\n        \n        image = tf.cast(image, tf.float32)\n        target = tf.cast(target, tf.float32)\n        \n        image = tf.ensure_shape(image, (224, 224, 3))\n        target = tf.ensure_shape(target, (28, 28, 5 + NUM_CLASSES))\n        yield image, target\n\ntrain_dataset = tf.data.Dataset.from_generator(\n    lambda:tf_data_generator(train_pairs),\n    output_signature=output_signature\n)\ntrain_dataset = train_dataset.shuffle(200).batch(8).repeat().prefetch(tf.data.AUTOTUNE)\nval_dataset = tf.data.Dataset.from_generator(\n    lambda:tf_data_generator(val_pairs),\n    output_signature=output_signature\n)\nval_dataset = val_dataset.batch(8).prefetch(tf.data.AUTOTUNE)\nfor images, targets in train_dataset.take(1):\n    print(images.shape)\n    print(targets.shape)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-09T16:40:39.254860Z","iopub.execute_input":"2026-01-09T16:40:39.255164Z","iopub.status.idle":"2026-01-09T16:40:44.446086Z","shell.execute_reply.started":"2026-01-09T16:40:39.255122Z","shell.execute_reply":"2026-01-09T16:40:44.445339Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from tensorflow.keras.layers import Input,Conv2D,MaxPooling2D,UpSampling2D,Concatenate,Conv2DTranspose\nfrom tensorflow.keras.layers import BatchNormalization,Activation\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.utils import plot_model\ndef custom_cnn(input_shape=(224,224,3)): \n    inputs = Input(shape=input_shape)\n    #block 1\n    x = Conv2D(16, (3, 3), padding='same', activation='relu')(inputs)\n    x = BatchNormalization()(x)\n    x = MaxPooling2D((2, 2))(x)\n    #block 2\n    x = Conv2D(32, (3, 3), padding='same', activation='relu')(x)\n    x = BatchNormalization()(x)\n    x = MaxPooling2D((2, 2))(x)\n    #block 3\n    x = Conv2D(64, (3, 3), padding='same', activation='relu')(x)\n    x = BatchNormalization()(x)\n    x = MaxPooling2D((2, 2))(x)\n    skip_features=x\n    #block 4\n    x = Conv2D(128, (3, 3), padding='same', activation='relu')(x)\n    x = BatchNormalization()(x)\n    x = MaxPooling2D((2, 2))(x)\n    model = Model(inputs=inputs, outputs=[x,skip_features], name=\"Custom_CNN_Backbone\")\n    return model","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-09T16:40:54.553758Z","iopub.execute_input":"2026-01-09T16:40:54.554059Z","iopub.status.idle":"2026-01-09T16:40:54.565664Z","shell.execute_reply.started":"2026-01-09T16:40:54.554033Z","shell.execute_reply":"2026-01-09T16:40:54.564780Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def detection_head(feature_maps,skip_features, num_classes):\n    x = UpSampling2D(2)(feature_maps)\n\n    skip = Conv2D(64, 1, padding='same', use_bias=False)(skip_features)\n    skip = BatchNormalization()(skip)\n    skip = Activation('relu')(skip)\n\n    x = Concatenate()([x, skip])\n    \n    x = Conv2D(256, 3, padding='same', use_bias=False)(x)\n    x = BatchNormalization()(x)\n    x = Activation('relu')(x)\n    output_channels = 5 + num_classes\n    outputs = Conv2D(\n        output_channels,\n        1,\n        padding='same',\n        activation=None\n    )(x)\n    return outputs","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-09T16:40:57.406135Z","iopub.execute_input":"2026-01-09T16:40:57.406996Z","iopub.status.idle":"2026-01-09T16:40:57.411770Z","shell.execute_reply.started":"2026-01-09T16:40:57.406962Z","shell.execute_reply":"2026-01-09T16:40:57.411082Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def object_model(input_shape,num_classes):\n    backbone=custom_cnn(input_shape)\n    feature_maps,skip_features=backbone.output\n    print(skip_features.shape)\n    print(feature_maps.shape)\n    detect=detection_head(feature_maps,skip_features,num_classes)\n    print(detect.shape)\n    return Model(\n        inputs=backbone.input,\n        outputs=detect,\n        name=\"Custom_CNN_Detector\"\n    )","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-09T16:41:01.196775Z","iopub.execute_input":"2026-01-09T16:41:01.197443Z","iopub.status.idle":"2026-01-09T16:41:01.201583Z","shell.execute_reply.started":"2026-01-09T16:41:01.197415Z","shell.execute_reply":"2026-01-09T16:41:01.200906Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def detection_loss(y_true, y_pred):\n    bbox_loss = tf.reduce_mean(tf.square(y_true[..., :4] - y_pred[..., :4]))\n    obj_loss = tf.keras.losses.binary_crossentropy(\n        y_true[..., 4], y_pred[..., 4], from_logits=True )\n    cls_loss = tf.keras.losses.sparse_categorical_crossentropy(\n        y_true[..., 5], y_pred[..., 5:], from_logits=True )\n\n    return bbox_loss + obj_loss + tf.reduce_mean(cls_loss)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-09T16:41:03.924401Z","iopub.execute_input":"2026-01-09T16:41:03.924670Z","iopub.status.idle":"2026-01-09T16:41:03.929488Z","shell.execute_reply.started":"2026-01-09T16:41:03.924647Z","shell.execute_reply":"2026-01-09T16:41:03.928629Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"model = object_model(input_shape=(224,224,3),num_classes=5)\nmodel.compile(\n    optimizer=tf.keras.optimizers.Adam(1e-3),\n    loss=detection_loss\n)\nmodel.summary()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-09T16:41:07.096606Z","iopub.execute_input":"2026-01-09T16:41:07.097053Z","iopub.status.idle":"2026-01-09T16:41:08.557064Z","shell.execute_reply.started":"2026-01-09T16:41:07.097019Z","shell.execute_reply":"2026-01-09T16:41:08.556481Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"model.fit(\n    train_dataset,\n    validation_data=val_dataset,\n    epochs=50,\n    steps_per_epoch=len(train_pairs)//8,\n    validation_steps=len(val_pairs)//8\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-09T16:41:20.865896Z","iopub.execute_input":"2026-01-09T16:41:20.866412Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def decode_predictions(pred, conf_thresh=0.05):\n    grid_size = pred.shape[0]\n    boxes = []\n\n    for i in range(grid_size):\n        for j in range(grid_size):\n            cell = pred[i, j]\n\n            obj_score = tf.sigmoid(cell[4])\n\n            if obj_score < conf_thresh:\n                continue\n                \n            tx = tf.sigmoid(cell[0])\n            ty = tf.sigmoid(cell[1])\n            tw = tf.sigmoid(cell[2])\n            th = tf.sigmoid(cell[3])\n\n            cx = (j + tx) / grid_size\n            cy = (i + ty) / grid_size\n            w  = tw\n            h  = th\n\n            xmin = tf.maximum(0.0,cx - w / 2)\n            ymin = tf.maximum(0.0,cy - h / 2)\n            xmax = tf.minimum(1.0,cx + w / 2)\n            ymax = tf.minimum(1.0,cy + h / 2)\n\n            class_id = tf.argmax(cell[5:])\n            class_score = tf.nn.softmax(cell[5:])[class_id]\n\n            score = obj_score * class_score\n\n            boxes.append([\n                xmin.numpy(),\n                ymin.numpy(),\n                xmax.numpy(),\n                ymax.numpy(),\n                score.numpy(),\n                int(class_id.numpy())\n            ])\n\n    return np.array(boxes)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-09T19:08:31.100822Z","iopub.execute_input":"2026-01-09T19:08:31.101555Z","iopub.status.idle":"2026-01-09T19:08:31.108356Z","shell.execute_reply.started":"2026-01-09T19:08:31.101523Z","shell.execute_reply":"2026-01-09T19:08:31.107477Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def apply_nms(boxes, iou_thresh=0.5):\n    if len(boxes) == 0:\n        return boxes\n\n    boxes_tf = tf.convert_to_tensor(boxes[:, :4], dtype=tf.float32)\n    scores_tf = tf.convert_to_tensor(boxes[:, 4], dtype=tf.float32)\n\n    selected = tf.image.non_max_suppression(\n        boxes_tf,\n        scores_tf,\n        max_output_size=100,\n        iou_threshold=iou_thresh\n    )\n\n    return boxes[selected.numpy()]","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def detect_objects_for_map(model, img, conf_thresh=0.05):\n    pred = model(img, training=False)[0].numpy()\n    decoded = decode_predictions(pred, conf_thresh)\n    final_boxes = apply_nms(decoded)\n    return final_boxes","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-09T19:08:36.644850Z","iopub.execute_input":"2026-01-09T19:08:36.645495Z","iopub.status.idle":"2026-01-09T19:08:36.649548Z","shell.execute_reply.started":"2026-01-09T19:08:36.645463Z","shell.execute_reply":"2026-01-09T19:08:36.648686Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"images = []\nannotations = []\nfor img_path, xml_path in val_pairs:\n    image, boxes, labels = load_image_and_labels(img_path, xml_path)\n    h, w, _ = image.shape\n    boxes = normalize_boxes(boxes, w, h)\n\n    image = cv2.resize(image, (224, 224))\n    image = image / 255.0\n\n    images.append(image.astype(np.float32))\n    annotations.append(boxes)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-09T19:08:39.942278Z","iopub.execute_input":"2026-01-09T19:08:39.943264Z","iopub.status.idle":"2026-01-09T19:08:59.474802Z","shell.execute_reply.started":"2026-01-09T19:08:39.943218Z","shell.execute_reply":"2026-01-09T19:08:59.474213Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"detections = []\nground_truths = []\n\nfor img, gt in zip(images, annotations):\n    img = np.expand_dims(img, axis=0)\n    det = detect_objects_for_map(model, img)\n\n    detections.append(det)\n    ground_truths.append(gt)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-09T19:09:04.766845Z","iopub.execute_input":"2026-01-09T19:09:04.767127Z","iopub.status.idle":"2026-01-09T19:29:44.816946Z","shell.execute_reply.started":"2026-01-09T19:09:04.767102Z","shell.execute_reply":"2026-01-09T19:29:44.816348Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def compute_iou(box1, box2):\n    x1, y1, x2, y2 = box1\n    x1g, y1g, x2g, y2g = box2\n\n    xi1 = max(x1, x1g)\n    yi1 = max(y1, y1g)\n    xi2 = min(x2, x2g)\n    yi2 = min(y2, y2g)\n    inter_area = max(0, xi2 - xi1) * max(0, yi2 - yi1)\n\n    box1_area = (x2 - x1) * (y2 - y1)\n    box2_area = (x2g - x1g) * (y2g - y1g)\n    union_area = box1_area + box2_area - inter_area\n\n    return inter_area / union_area","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-09T19:29:51.473773Z","iopub.execute_input":"2026-01-09T19:29:51.474492Z","iopub.status.idle":"2026-01-09T19:29:51.479280Z","shell.execute_reply.started":"2026-01-09T19:29:51.474460Z","shell.execute_reply":"2026-01-09T19:29:51.478594Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from sklearn.metrics import average_precision_score\ndef compute_map(detections, annotations, iou_threshold=0.5):\n    aps = []\n    for det, ann in zip(detections, annotations):\n        if len(ann) == 0:\n            continue  \n\n        tp = 0\n        fp = 0\n        used = [False] * len(ann)\n\n        for d in det:\n            matched = False\n            for idx, a in enumerate(ann):\n                if used[idx]:\n                    continue  \n                iou = compute_iou(d[:4], a)\n                if iou >= iou_threshold:\n                    tp += 1\n                    used[idx] = True\n                    matched = True\n                    break\n            if not matched:\n                fp += 1  \n\n        precision = tp / (tp + fp) if (tp + fp) > 0 else 0\n        recall = tp / len(ann) if len(ann) > 0 else 0\n        aps.append(precision * recall)\n\n    return np.mean(aps) if len(aps) > 0 else 0\n\nmAP = compute_map(detections, ground_truths)\nprint(f\"Mean Average Precision (mAP): {mAP:.4f}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-09T19:29:55.120862Z","iopub.execute_input":"2026-01-09T19:29:55.121168Z","iopub.status.idle":"2026-01-09T19:29:55.189296Z","shell.execute_reply.started":"2026-01-09T19:29:55.121123Z","shell.execute_reply":"2026-01-09T19:29:55.188570Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import time\nstart = time.time()\nmodel.predict(val_dataset.take(10))\nfps = 10 / (time.time() - start)\nprint(\"FPS:\", fps)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-09T19:30:08.157419Z","iopub.execute_input":"2026-01-09T19:30:08.158130Z","iopub.status.idle":"2026-01-09T19:30:09.469838Z","shell.execute_reply.started":"2026-01-09T19:30:08.158100Z","shell.execute_reply":"2026-01-09T19:30:09.468930Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"model.save(\"custom_detector.h5\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-09T19:30:24.780539Z","iopub.execute_input":"2026-01-09T19:30:24.780828Z","iopub.status.idle":"2026-01-09T19:30:24.869587Z","shell.execute_reply.started":"2026-01-09T19:30:24.780804Z","shell.execute_reply":"2026-01-09T19:30:24.869031Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import cv2\nimport numpy as np\n\ndef run_detection(model, image_path, conf_thresh=0.10):\n    img = cv2.imread(image_path)\n    h, w, _ = img.shape\n\n    img_resized = cv2.resize(img, (224, 224))\n    img_input = img_resized / 255.0\n    img_input = np.expand_dims(img_input, axis=0)\n\n    boxes = detect_objects_for_map(model, img_input, conf_thresh)\n\n    for box in boxes:\n        xmin, ymin, xmax, ymax, score, cls = box\n        x1 = int(xmin * w)\n        y1 = int(ymin * h)\n        x2 = int(xmax * w)\n        y2 = int(ymax * h)\n\n        cv2.rectangle(img, (x1, y1), (x2, y2), (0,255,0), 2)\n        cv2.putText(\n            img,\n            f\"Class {cls} | {score:.2f}\",\n            (x1, y1 - 5),\n            cv2.FONT_HERSHEY_SIMPLEX,\n            0.5,\n            (0,255,0),\n            1\n        )\n    return img ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-09T19:45:48.010571Z","iopub.execute_input":"2026-01-09T19:45:48.010827Z","iopub.status.idle":"2026-01-09T19:45:48.017184Z","shell.execute_reply.started":"2026-01-09T19:45:48.010806Z","shell.execute_reply":"2026-01-09T19:45:48.016432Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import matplotlib.pyplot as plt\n\nimg_path = \"/kaggle/input/pascal-voc-2012-dataset/VOC2012_test/VOC2012_test/JPEGImages/2008_000029.jpg\"\n\noutput = run_detection(model, img_path, conf_thresh=0.15)\n\nplt.figure(figsize=(8,8))\nplt.imshow(cv2.cvtColor(output, cv2.COLOR_BGR2RGB))\nplt.axis(\"off\")\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-09T19:55:26.019566Z","iopub.execute_input":"2026-01-09T19:55:26.020137Z","iopub.status.idle":"2026-01-09T19:55:26.656441Z","shell.execute_reply.started":"2026-01-09T19:55:26.020107Z","shell.execute_reply":"2026-01-09T19:55:26.655420Z"}},"outputs":[],"execution_count":null}]}